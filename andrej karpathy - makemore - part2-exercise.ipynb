{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] \n",
      "\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} \n",
      "\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "print(chars, '\\n')\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "print(stoi , '\\n')\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos , '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(words , block_size):\n",
    "    X , Y = [] , [] \n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        word =  word + '.' # append a end character to word, so the generator is trained to stop\n",
    "        for ch in word:\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def trainDevAndTestSplit(words , block_size):\n",
    "    random.seed(42)\n",
    "    random.shuffle(words)\n",
    "    n1 = int(0.8 * len(words))\n",
    "    n2 = int(0.9 * len(words))\n",
    "    xtr, ytr = buildDataset(words[:n1], block_size)\n",
    "    xdev, ydev = buildDataset(words[n1:n2], block_size)\n",
    "    xtest, ytest = buildDataset(words[n2:], block_size)\n",
    "    return torch.tensor(xtr), torch.tensor(ytr), torch.tensor(xdev), torch.tensor(ydev), torch.tensor(xtest), torch.tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "xtr, ytr, xdev, ydev, xtest, ytest = trainDevAndTestSplit(words, block_size)\n",
    "\n",
    "print(xtr.shape, ytr.shape)\n",
    "print(xdev.shape, ydev.shape)\n",
    "print(xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "character_embedding_size = 10\n",
    "layer1_size = 200\n",
    "\n",
    "C = torch.randn((27 , character_embedding_size) , generator = g) # character embedding\n",
    "W1 = torch.randn((character_embedding_size * block_size , layer1_size) , generator = g)\n",
    "b1 = torch.randn(layer1_size , generator = g)\n",
    "W2 = torch.randn((layer1_size , 27) , generator = g)\n",
    "b2 = torch.randn(27 , generator = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters  11897\n"
     ]
    }
   ],
   "source": [
    "network_parameters = [C , W1 , b1 , W2, b2]\n",
    "print('Total no of parameters ' , sum(p.nelement() for p in network_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in network_parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardAndBackwardPass(x , y , batch_size , lr):\n",
    "    ix_batch = torch.randint(0 , x.shape[0], (batch_size, ))\n",
    "    x_batch = x[ix_batch]\n",
    "    embed_batch = C[x_batch].flatten(1)\n",
    "    \n",
    "    y_batch = y[ix_batch]\n",
    "    \n",
    "    h0 = embed_batch @ W1 + b1\n",
    "    \n",
    "    h1 = torch.tanh(h0)\n",
    "    \n",
    "    logits = h1 @ W2 + b2\n",
    "    \n",
    "    loss = F.cross_entropy(logits , y_batch)\n",
    "    \n",
    "    # backward pass \n",
    "    for p in network_parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # update grad\n",
    "    for p in network_parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    return loss.item()\n",
    "\n",
    "def trainNetwork(x, y, batch_size , epochs , lr):\n",
    "    loss_i = []\n",
    "    iter_i = []\n",
    "    \n",
    "    later_training_epoch = int(0.9 * epochs)\n",
    "    \n",
    "    for i in range(0,epochs):\n",
    "        loss = None\n",
    "        \n",
    "        if i > later_training_epoch:\n",
    "            learning_rate_decay = lr * ( 1 / (1 + 0.01 * i)) #change the learning rate for final steps of training\n",
    "            loss = forwardAndBackwardPass(x, y, batch_size , learning_rate_decay)\n",
    "        else:\n",
    "            loss = forwardAndBackwardPass(x, y, batch_size , lr)\n",
    "            \n",
    "        loss_i.append(loss)\n",
    "        iter_i.append(i)\n",
    "    return loss_i , iter_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses , iters = trainNetwork(xtr , ytr , 64 , 200000 , 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8942257165908813,\n",
       " 1.7774536609649658,\n",
       " 2.371021032333374,\n",
       " 2.033993721008301,\n",
       " 1.8186655044555664,\n",
       " 2.0350120067596436,\n",
       " 2.162750482559204,\n",
       " 2.1924843788146973,\n",
       " 2.0609397888183594,\n",
       " 2.270714282989502]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x224eb00f208>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iters , losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(x,y):\n",
    "    with torch.no_grad():\n",
    "        emb = C[x].flatten(1)\n",
    "        h0 = emb @ W1 + b1 \n",
    "        h1 = torch.tanh(h0)\n",
    "        logits = h1 @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set loss  tensor(2.1006)\n"
     ]
    }
   ],
   "source": [
    "print('Training set loss ' , evaluate_loss(xtr,ytr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set loss  tensor(2.1540)\n"
     ]
    }
   ],
   "source": [
    "print('Validation set loss ' , evaluate_loss(xdev,ydev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set loss  tensor(2.1509)\n"
     ]
    }
   ],
   "source": [
    "print('test set loss ' , evaluate_loss(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "amori.\n",
      "killimri.\n",
      "reytyn.\n",
      "kanden.\n",
      "jazhube.\n",
      "delyah.\n",
      "jareei.\n",
      "nellara.\n",
      "chaiivon.\n",
      "leigh.\n",
      "ham.\n",
      "pris.\n",
      "quinn.\n",
      "shon.\n",
      "malian.\n",
      "quince.\n",
      "madiaryxix.\n",
      "kael.\n",
      "ivrailey.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])].flatten(1) # (1,block_size,d)\n",
    "        h0 = emb @ W1 + b1 \n",
    "        h1 = torch.tanh(h0)\n",
    "        logits = h1 @ W2 + b2\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E02: I was not careful with the intialization of the network in this video.\n",
    "#(1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform?\n",
    "# What loss do we achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[xtr].flatten(1) # (1,block_size,d)\n",
    "h0 = emb @ W1 + b1\n",
    "h1 = torch.tanh(h0)\n",
    "logits = h1 @ torch.zeros(W2.shape) + torch.zeros(b2.shape)\n",
    "probs = F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000,  0.9772, -0.9916,  ...,  0.9998,  0.9995,  0.9965],\n",
       "        [-0.0966, -0.8553, -0.9988,  ..., -1.0000,  0.9999,  1.0000],\n",
       "        [-0.9882, -0.9923, -1.0000,  ..., -0.9984,  0.9932,  0.8910],\n",
       "        ...,\n",
       "        [ 0.7192, -0.9998, -0.9166,  ..., -0.8532, -1.0000, -0.9999],\n",
       "        [-0.6699, -0.9996, -1.0000,  ..., -0.9999, -0.9519,  0.8732],\n",
       "        [-0.9978, -1.0000, -1.0000,  ..., -0.7131, -0.9994, -0.9633]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3679, 2.6570, 0.3710, 2.6354, 2.7182, 2.7183, 2.7151, 0.3748, 0.3679,\n",
       "        0.3679, 2.7150, 0.3679, 0.3679, 0.3696, 2.6969, 2.5035, 0.3684, 0.3709,\n",
       "        2.7095, 2.7014, 0.3703, 0.3681, 2.6198, 2.6183, 2.7142, 2.6122, 2.5759,\n",
       "        2.7183, 0.3679, 2.7167, 2.5836, 0.3679, 2.7183, 2.7181, 0.3679, 0.3682,\n",
       "        0.3786, 0.3679, 2.7180, 2.7183, 2.5982, 0.4012, 2.7182, 0.3679, 2.5878,\n",
       "        2.5818, 0.3679, 0.3679, 2.6277, 2.7175, 0.3705, 0.3838, 2.6769, 2.7182,\n",
       "        2.7171, 0.3680, 0.3680, 0.3679, 2.7182, 2.5926, 0.3703, 0.3706, 2.6438,\n",
       "        2.6018, 2.6747, 2.6524, 0.3679, 0.3689, 2.7172, 0.3679, 2.7182, 2.7182,\n",
       "        2.5822, 0.3722, 0.3731, 2.7148, 2.6980, 2.7182, 2.7183, 0.3679, 2.7183,\n",
       "        0.3679, 0.3679, 2.7183, 0.3715, 2.6210, 0.3681, 2.5435, 2.7183, 0.4126,\n",
       "        2.7183, 2.6646, 2.6423, 2.7175, 2.7183, 0.3778, 0.3679, 0.3679, 2.2381,\n",
       "        0.3679, 0.3682, 0.3679, 0.3679, 2.7183, 2.6771, 2.7183, 0.3680, 0.3947,\n",
       "        0.3768, 0.3679, 0.3784, 2.7152, 0.3809, 2.6581, 0.3761, 2.6445, 0.3688,\n",
       "        0.3709, 2.4739, 0.3679, 0.3679, 0.3815, 2.7160, 2.7180, 2.6706, 0.3694,\n",
       "        2.7159, 0.3792, 0.4093, 2.7166, 2.6940, 2.7157, 2.6049, 0.3679, 0.3699,\n",
       "        2.7163, 2.4510, 0.3788, 2.6068, 2.7144, 0.3924, 2.7045, 2.6952, 0.3711,\n",
       "        2.7177, 0.3679, 2.7183, 2.4591, 2.7183, 2.7135, 2.7183, 0.3731, 0.3680,\n",
       "        2.6637, 0.3679, 0.3679, 0.3699, 0.3795, 0.3679, 0.3683, 2.7163, 2.7179,\n",
       "        0.3734, 0.3920, 0.3705, 0.3843, 0.3688, 0.4001, 0.3679, 2.7134, 2.7183,\n",
       "        2.7164, 2.7148, 0.3679, 2.6008, 2.4824, 2.6988, 0.3679, 0.3760, 0.3681,\n",
       "        2.7177, 0.3679, 0.3694, 2.7182, 0.3682, 2.7164, 0.3679, 0.4140, 2.7183,\n",
       "        2.5350, 2.7174, 2.7183, 2.7163, 2.7183, 0.3934, 2.7156, 2.7175, 2.7176,\n",
       "        2.7170, 2.7089], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1[0].exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000,  0.9772, -0.9916,  0.9690,  1.0000,  1.0000,  0.9988, -0.9814,\n",
       "        -0.9999, -0.9999,  0.9988, -1.0000, -0.9998, -0.9954,  0.9921,  0.9177,\n",
       "        -0.9985, -0.9919,  0.9968,  0.9938, -0.9935, -0.9995,  0.9631,  0.9625,\n",
       "         0.9985,  0.9602,  0.9462,  1.0000, -1.0000,  0.9994,  0.9492, -1.0000,\n",
       "         1.0000,  0.9999, -1.0000, -0.9991, -0.9713, -0.9999,  0.9999,  1.0000,\n",
       "         0.9548, -0.9133,  1.0000, -0.9999,  0.9508,  0.9485, -1.0000, -0.9999,\n",
       "         0.9661,  0.9997, -0.9929, -0.9577,  0.9847,  1.0000,  0.9996, -0.9997,\n",
       "        -0.9997, -1.0000,  1.0000,  0.9527, -0.9936, -0.9927,  0.9722,  0.9562,\n",
       "         0.9838,  0.9755, -0.9999, -0.9971,  0.9996, -0.9998,  1.0000,  1.0000,\n",
       "         0.9486, -0.9883, -0.9858,  0.9987,  0.9925,  1.0000,  1.0000, -1.0000,\n",
       "         1.0000, -1.0000, -1.0000,  1.0000, -0.9903,  0.9636, -0.9993,  0.9335,\n",
       "         1.0000, -0.8853,  1.0000,  0.9800,  0.9717,  0.9997,  1.0000, -0.9733,\n",
       "        -0.9998, -0.9998,  0.8056, -1.0000, -0.9991, -0.9999, -1.0000,  1.0000,\n",
       "         0.9847,  1.0000, -0.9998, -0.9296, -0.9759, -1.0000, -0.9717,  0.9988,\n",
       "        -0.9653,  0.9776, -0.9778,  0.9725, -0.9976, -0.9918,  0.9058, -1.0000,\n",
       "        -1.0000, -0.9637,  0.9992,  0.9999,  0.9823, -0.9958,  0.9991, -0.9697,\n",
       "        -0.8933,  0.9994,  0.9910,  0.9990,  0.9574, -1.0000, -0.9946,  0.9993,\n",
       "         0.8965, -0.9708,  0.9581,  0.9986, -0.9355,  0.9949,  0.9915, -0.9912,\n",
       "         0.9998, -0.9999,  1.0000,  0.8998,  1.0000,  0.9982,  1.0000, -0.9860,\n",
       "        -0.9997,  0.9797, -0.9999, -1.0000, -0.9945, -0.9689, -0.9999, -0.9990,\n",
       "         0.9993,  0.9998, -0.9852, -0.9364, -0.9930, -0.9564, -0.9974, -0.9161,\n",
       "        -1.0000,  0.9982,  1.0000,  0.9993,  0.9987, -1.0000,  0.9558,  0.9092,\n",
       "         0.9928, -0.9999, -0.9783, -0.9993,  0.9998, -1.0000, -0.9959,  1.0000,\n",
       "        -0.9991,  0.9993, -1.0000, -0.8820,  1.0000,  0.9302,  0.9997,  1.0000,\n",
       "         0.9993,  1.0000, -0.9330,  0.9990,  0.9997,  0.9998,  0.9995,  0.9965],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[1] # prodicted prob are uniform. we did this by setting w2 and b2 = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(logits , ytr) # calculating the loss with the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
