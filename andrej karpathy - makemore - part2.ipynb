{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our previous model, we only took 1 character of context (bigram) so we got not so good results\n",
    "# if we take 2 characters as context , we will grow into 27 * 27 context matrix \n",
    "# if we take 3 characters as context , we will grow into 27 * 27 * 27 context matrix \n",
    "# it will be extremely difficult to keep track of these counts\n",
    "# we are following https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] \n",
      "\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} \n",
      "\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "print(chars, '\\n')\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "print(stoi , '\\n')\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos , '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        [ 5, 13, 13],\n",
      "        [13, 13,  1]])\n",
      "tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:1]:\n",
    "    print(w)\n",
    "    context = [0] * block_size # the number of characters we are looking at\n",
    "    for ch in w + '.': # add the end char '.' to word\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    context = [0] * block_size # the number of characters we are looking at\n",
    "    for ch in w + '.': # add the end char '.' to word\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# from 5 words, we created 32 examples. each input is 3 integers. and we have 32 labels for each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding lookup table of lower dimension. In original paper, the authors cram 17000 words into 30 dimension space.\n",
    "# we are working at char level, so we have 27 characters (26 + '.'). let's cram them into 2 dim space\n",
    "C = torch.randn((27,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3]) torch.Size([27, 2])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape # we get the embedding vector for every [32,3] input. power of pytorch indexing.\n",
    "# how does this work ? suppose we take an input X[1]. That's a tensor [0,0,5]. when we pass it to C, we are basically\n",
    "# asking C to give us the data it has at index 0 , index 0 and index 5\n",
    "# so it gives us a 3,2 output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425],\n",
       "        [-0.0515, -0.2759]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so the embedding is \n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create the hidden layer\n",
    "# our input is 3 dimension , and each has embedding in two dimension\n",
    "# so this layer will have 6 dimensions input\n",
    "W1 = torch.randn((6,100)) # 100 is the arbitrary number of neurons we want to have \n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-0.0515, -0.2759],\n",
      "        [ 1.0071, -0.7074],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.7338, -0.7256],\n",
      "        [-0.5880, -0.2260],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [-1.5788,  1.3383],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [-1.5919, -1.4285],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-0.0986, -0.8607],\n",
      "        [-0.0515, -0.2759],\n",
      "        [-0.5880, -0.2260],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.5919, -1.4285],\n",
      "        [ 0.7338, -0.7256],\n",
      "        [ 1.2747,  1.0312],\n",
      "        [-0.7802, -1.5185]])\n",
      "tensor([[-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-0.0515, -0.2759],\n",
      "        [ 1.0071, -0.7074],\n",
      "        [ 1.0071, -0.7074],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.7338, -0.7256],\n",
      "        [-0.5880, -0.2260],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [-1.5788,  1.3383],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-1.5788,  1.3383],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [-1.5919, -1.4285],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-0.0986, -0.8607],\n",
      "        [-0.0515, -0.2759],\n",
      "        [-0.5880, -0.2260],\n",
      "        [-0.5880, -0.2260],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.5919, -1.4285],\n",
      "        [ 0.7338, -0.7256],\n",
      "        [ 1.2747,  1.0312],\n",
      "        [-0.7802, -1.5185],\n",
      "        [ 0.8748, -0.3433]])\n",
      "tensor([[-1.3275,  1.9425],\n",
      "        [-0.0515, -0.2759],\n",
      "        [ 1.0071, -0.7074],\n",
      "        [ 1.0071, -0.7074],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.7338, -0.7256],\n",
      "        [-0.5880, -0.2260],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [-1.5788,  1.3383],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-1.5788,  1.3383],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-1.3275,  1.9425],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [-1.5919, -1.4285],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-0.0986, -0.8607],\n",
      "        [-0.0515, -0.2759],\n",
      "        [-0.5880, -0.2260],\n",
      "        [-0.5880, -0.2260],\n",
      "        [ 0.5097, -0.6473],\n",
      "        [-1.3275,  1.9425],\n",
      "        [-1.5919, -1.4285],\n",
      "        [ 0.7338, -0.7256],\n",
      "        [ 1.2747,  1.0312],\n",
      "        [-0.7802, -1.5185],\n",
      "        [ 0.8748, -0.3433],\n",
      "        [ 0.5097, -0.6473]])\n"
     ]
    }
   ],
   "source": [
    "# emb @ W1 wont work because the dims dont match\n",
    "# what we want to do is retrive the 3 parts of the emb (3,2) [3 represents the one hot enc of char and 2\n",
    "# represents the dimension of embedding] and concatenate them\n",
    "\n",
    "print(emb[: , 0 , :]) # embs of char 1\n",
    "print(emb[: , 1 , :]) # embs of char 2\n",
    "print(emb[: , 2 , :]) # emb of char 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -0.0515, -0.2759],\n",
       "        [-1.3275,  1.9425, -0.0515, -0.2759,  1.0071, -0.7074],\n",
       "        [-0.0515, -0.2759,  1.0071, -0.7074,  1.0071, -0.7074],\n",
       "        [ 1.0071, -0.7074,  1.0071, -0.7074,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425,  0.7338, -0.7256],\n",
       "        [-1.3275,  1.9425,  0.7338, -0.7256, -0.5880, -0.2260],\n",
       "        [ 0.7338, -0.7256, -0.5880, -0.2260,  0.8748, -0.3433],\n",
       "        [-0.5880, -0.2260,  0.8748, -0.3433, -1.5788,  1.3383],\n",
       "        [ 0.8748, -0.3433, -1.5788,  1.3383,  0.8748, -0.3433],\n",
       "        [-1.5788,  1.3383,  0.8748, -0.3433,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425,  0.5097, -0.6473, -1.5788,  1.3383],\n",
       "        [ 0.5097, -0.6473, -1.5788,  1.3383,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425,  0.8748, -0.3433],\n",
       "        [-1.3275,  1.9425,  0.8748, -0.3433, -1.5919, -1.4285],\n",
       "        [ 0.8748, -0.3433, -1.5919, -1.4285,  0.5097, -0.6473],\n",
       "        [-1.5919, -1.4285,  0.5097, -0.6473, -0.0986, -0.8607],\n",
       "        [ 0.5097, -0.6473, -0.0986, -0.8607, -0.0515, -0.2759],\n",
       "        [-0.0986, -0.8607, -0.0515, -0.2759, -0.5880, -0.2260],\n",
       "        [-0.0515, -0.2759, -0.5880, -0.2260, -0.5880, -0.2260],\n",
       "        [-0.5880, -0.2260, -0.5880, -0.2260,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.5919, -1.4285],\n",
       "        [-1.3275,  1.9425, -1.5919, -1.4285,  0.7338, -0.7256],\n",
       "        [-1.5919, -1.4285,  0.7338, -0.7256,  1.2747,  1.0312],\n",
       "        [ 0.7338, -0.7256,  1.2747,  1.0312, -0.7802, -1.5185],\n",
       "        [ 1.2747,  1.0312, -0.7802, -1.5185,  0.8748, -0.3433],\n",
       "        [-0.7802, -1.5185,  0.8748, -0.3433,  0.5097, -0.6473]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then we want to concatenate them  to create a layer of all embeddings for those 3 characters\n",
    "torch.cat([emb[: , 0 , :] , emb[: , 1 , :] , emb[: , 2 , :]] , 1)\n",
    "# BASICALLY IF THE FIRST INPUT IS ... -> ONE HOT ENCODING IS --> [0 ,0 ,0] --> EMBED FOR '.' IS [0.5291, -1.1664] \n",
    "# SO WE HAVE  [0.5291, -1.1664 , 0.5291, -1.1664 , 0.5291, -1.1664 ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use unbind to do this\n",
    "print(emb.shape)\n",
    "torch.unbind(emb, 1) # Returns a tuple of all slices along a given dimension, already without it\n",
    "torch.cat(torch.unbind(emb, 1),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -0.0515, -0.2759],\n",
       "        [-1.3275,  1.9425, -0.0515, -0.2759,  1.0071, -0.7074],\n",
       "        [-0.0515, -0.2759,  1.0071, -0.7074,  1.0071, -0.7074],\n",
       "        [ 1.0071, -0.7074,  1.0071, -0.7074,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425,  0.7338, -0.7256],\n",
       "        [-1.3275,  1.9425,  0.7338, -0.7256, -0.5880, -0.2260],\n",
       "        [ 0.7338, -0.7256, -0.5880, -0.2260,  0.8748, -0.3433],\n",
       "        [-0.5880, -0.2260,  0.8748, -0.3433, -1.5788,  1.3383],\n",
       "        [ 0.8748, -0.3433, -1.5788,  1.3383,  0.8748, -0.3433],\n",
       "        [-1.5788,  1.3383,  0.8748, -0.3433,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425,  0.5097, -0.6473, -1.5788,  1.3383],\n",
       "        [ 0.5097, -0.6473, -1.5788,  1.3383,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425,  0.8748, -0.3433],\n",
       "        [-1.3275,  1.9425,  0.8748, -0.3433, -1.5919, -1.4285],\n",
       "        [ 0.8748, -0.3433, -1.5919, -1.4285,  0.5097, -0.6473],\n",
       "        [-1.5919, -1.4285,  0.5097, -0.6473, -0.0986, -0.8607],\n",
       "        [ 0.5097, -0.6473, -0.0986, -0.8607, -0.0515, -0.2759],\n",
       "        [-0.0986, -0.8607, -0.0515, -0.2759, -0.5880, -0.2260],\n",
       "        [-0.0515, -0.2759, -0.5880, -0.2260, -0.5880, -0.2260],\n",
       "        [-0.5880, -0.2260, -0.5880, -0.2260,  0.5097, -0.6473],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.3275,  1.9425],\n",
       "        [-1.3275,  1.9425, -1.3275,  1.9425, -1.5919, -1.4285],\n",
       "        [-1.3275,  1.9425, -1.5919, -1.4285,  0.7338, -0.7256],\n",
       "        [-1.5919, -1.4285,  0.7338, -0.7256,  1.2747,  1.0312],\n",
       "        [ 0.7338, -0.7256,  1.2747,  1.0312, -0.7802, -1.5185],\n",
       "        [ 1.2747,  1.0312, -0.7802, -1.5185,  0.8748, -0.3433],\n",
       "        [-0.7802, -1.5185,  0.8748, -0.3433,  0.5097, -0.6473]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# even better way\n",
    "emb.view(32,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the matrix mul works\n",
    "h = emb.view((emb.shape[0],6)) @ W1 + b1 # we dont want to hardcode 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the non linearity\n",
    "h = torch.tanh(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer\n",
    "W2 = torch.randn((100,27)) # now we need 27 outputs -> one for each char\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits\n",
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2107e-01, 4.2062e-07, 2.7437e-01, 1.8250e-08, 2.7490e-09, 2.4591e-04,\n",
       "        5.1281e-11, 2.0385e-10, 2.2215e-05, 6.2011e-13, 5.6370e-08, 2.8434e-04,\n",
       "        1.1824e-10, 2.0814e-04, 5.0267e-11, 4.1479e-02, 7.9145e-08, 3.1619e-12,\n",
       "        3.2873e-18, 2.9147e-08, 6.6476e-01, 1.6828e-04, 1.0758e-05, 2.6288e-10,\n",
       "        2.9897e-06, 7.2773e-10, 3.8646e-10, 1.1974e-08, 4.2854e-10, 6.3489e-01,\n",
       "        3.1334e-07, 1.4438e-10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32) , Y] # get the prob of target from prob matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.8819)\n"
     ]
    }
   ],
   "source": [
    "neg_log_loss = -prob[torch.arange(32) , Y].log().mean()\n",
    "print(neg_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.8819)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can do the count, prob and loss in one single method in pytorch. we get the same loss\n",
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3]) torch.Size([32])\n",
      "Total number of parameters :  3481\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "print(X.shape, Y.shape)\n",
    "C = torch.randn((27, 2), generator=g) # embedding of 27 chars in 2D\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print('Total number of parameters : ' , sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.76971435546875\n",
      "13.656400680541992\n",
      "11.298768997192383\n",
      "9.452457427978516\n",
      "7.984262466430664\n",
      "6.891321182250977\n",
      "6.100014686584473\n",
      "5.452036380767822\n",
      "4.898151874542236\n",
      "4.414663791656494\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    #forward pass\n",
    "    emb = C[X] # [32,3,2]\n",
    "    h = emb.view(-1,6) @ W1 + b1\n",
    "    h = torch.tanh(h)\n",
    "    logits = h @ W2 + b2 # [32 , 27]\n",
    "    loss = F.cross_entropy(logits , Y)\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update grad \n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 15, 11, 25, 26,  1, 14, 13, 31, 25, 22,  5,  4, 15, 24, 25, 24, 10,\n",
       "        20, 16, 23, 14, 12, 24,  1, 29, 28,  8, 16, 10, 23, 29])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this was only for words , and our input for those 5 words became 32 , 3 (after one hot encoding)\n",
    "# as we take the entire dataset , the input will be even bigger\n",
    "# in practice , we like to optimize loss over batches of bata , not the entire data set\n",
    "# randomly select a mini batch of data\n",
    "\n",
    "torch.randint(0,5, (32,)) # give 32 ints between 0 and 5\n",
    "\n",
    "torch.randint(0,X.shape[0], (32,)) # give 32 ints between 0 and number of inputs in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.061162948608398\n",
      "2.5981428623199463\n",
      "2.328188180923462\n",
      "3.810340404510498\n",
      "3.112499475479126\n",
      "2.28739070892334\n",
      "1.9184293746948242\n",
      "2.216069221496582\n",
      "3.096294641494751\n",
      "1.6646702289581299\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # minibatch \n",
    "    #forward pass\n",
    "    emb = C[X[ix]] # [32,3,2]\n",
    "    h = emb.view(-1,6) @ W1 + b1\n",
    "    h = torch.tanh(h)\n",
    "    logits = h @ W2 + b2 # [32 , 27]\n",
    "    loss = F.cross_entropy(logits , Y[ix])\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update grad \n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# in real life we will have the dataset be split into train , dev and test. now we will take the entire data\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "              #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters :  11897\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 10), generator=g) # embedding of 27 chars in 10 dimensions insted of 2 dim\n",
    "W1 = torch.randn((30, 200), generator=g) # 3 characters, each of 10 dim embedding, so 30 dim input ... 200 neurons\n",
    "b1 = torch.randn(200, generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print('Total number of parameters : ' , sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200000):\n",
    "  \n",
    "  # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "  \n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 10)\n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 200)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "  \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # learning rate decay over time\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cdf84aab88>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7203383445739746,\n",
       " 2.1895627975463867,\n",
       " 1.9012714624404907,\n",
       " 1.7211579084396362,\n",
       " 1.8986085653305054,\n",
       " 1.7795028686523438,\n",
       " 1.7319889068603516,\n",
       " 1.9536006450653076,\n",
       " 2.318521738052368,\n",
       " 2.463441848754883,\n",
       " 2.3640260696411133,\n",
       " 2.5725903511047363,\n",
       " 2.09915828704834,\n",
       " 2.2030224800109863,\n",
       " 2.140836238861084,\n",
       " 2.146395206451416,\n",
       " 2.20281720161438,\n",
       " 1.9810020923614502,\n",
       " 1.955871820449829,\n",
       " 1.8515732288360596]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossi[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1351, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate loss on training\n",
    "emb = C[Xtr] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100) \n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1817, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate validation loss\n",
    "emb = C[Xdev] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "amille.\n",
      "khi.\n",
      "mri.\n",
      "rehiyah.\n",
      "cassie.\n",
      "mahith.\n",
      "delynn.\n",
      "jareei.\n",
      "nellara.\n",
      "chaiir.\n",
      "kaleigh.\n",
      "ham.\n",
      "pori.\n",
      "quinton.\n",
      "lilea.\n",
      "jamilio.\n",
      "jermadiarisi.\n",
      "fine.\n",
      "pinsley.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
