{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] \n",
      "\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} \n",
      "\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} \n",
      "\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "print(chars, '\\n')\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "print(stoi , '\\n')\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos , '\\n')\n",
    "vocab_size = len(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])                               #80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])                           #10%\n",
    "Xte, Yte = build_dataset(words[n2:])                               #10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in total  11897\n"
     ]
    }
   ],
   "source": [
    "# always run this cell (initialization) before optimization\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) \n",
    "b1 = torch.randn(n_hidden,                        generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g)\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2 , b2]\n",
    "\n",
    "print('Number of parameters in total ' , sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 27.8817\n",
      "  10000/ 200000: 2.8240\n",
      "  20000/ 200000: 2.5163\n",
      "  30000/ 200000: 2.8836\n",
      "  40000/ 200000: 2.0655\n",
      "  50000/ 200000: 2.4969\n",
      "  60000/ 200000: 2.4992\n",
      "  70000/ 200000: 2.0261\n",
      "  80000/ 200000: 2.4461\n",
      "  90000/ 200000: 2.2756\n",
      " 100000/ 200000: 2.0263\n",
      " 110000/ 200000: 2.3349\n",
      " 120000/ 200000: 1.8987\n",
      " 130000/ 200000: 2.3830\n",
      " 140000/ 200000: 2.1785\n",
      " 150000/ 200000: 2.1847\n",
      " 160000/ 200000: 2.0797\n",
      " 170000/ 200000: 1.8553\n",
      " 180000/ 200000: 1.9629\n",
      " 190000/ 200000: 1.8522\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32 # to take a minibatch of data \n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb , Yb = Xtr[ix] , Ytr[ix] #batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] #embed chars into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) #concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "  \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # learning rate decay over time\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2afc401def0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1217586994171143\n",
      "val 2.162574052810669\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    \n",
    "    emb = C[x] # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182625, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "amille.\n",
      "khirmrex.\n",
      "taty.\n",
      "skanden.\n",
      "jazonel.\n",
      "den.\n",
      "arci.\n",
      "aqui.\n",
      "ner.\n",
      "kentzieiivia.\n",
      "legy.\n",
      "dham.\n",
      "jorn.\n",
      "quintis.\n",
      "lilea.\n",
      "jadii.\n",
      "waythoniearynix.\n",
      "kaeliigh.\n",
      "boe.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the very first iteration, our loss is 27. and then we go to 2 .. it means our initialization is very bad\n",
    "# at init, the prob distribiton should assign equal prob to all\n",
    "# (because we have no reason to belive one is more likely than other)\n",
    "# prob = 1/27 \n",
    "# loss = 3.29 (if you take the nll of prob 1/27)\n",
    "# at init, the network is very confidently wrong.\n",
    "# we want the logits to be equal (close to 0), so that the prob coming out post softmax is roughly same for all.\n",
    "# logits = h @ W2 (ignore b for now)\n",
    "# logits = h @ (W2 * 0.1) \n",
    "# we dont multiple W2 by 0 because that causes \n",
    "# let's test . first we should run the initialization cell (run the initialization cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.0360\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32 # to take a minibatch of data \n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb , Yb = Xtr[ix] , Ytr[ix] #batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] #embed chars into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) #concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    h = torch.tanh(hpreact)  # hidden layer\n",
    "    logits = h @ (W2 * 0.1)  + b2 * 0\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "  \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # learning rate decay over time\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2afc399e6a0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we start with a much smaller loss now (around 4)! \n",
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let's get make some improvements on the hidden layer. Let's visuaize h\n",
    "plt.hist(h.view(-1).tolist() , 50);\n",
    "# show's that the tanh is very very active. most of the values fall in -1 and +1 . h = torch.tanh(hpreact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(hpreact.view(-1).tolist() , 50); # similarly we see the distribution of hpreact is very broad (take -40 to 40 \n",
    "#which is very extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2afc4426ac8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# during backprop, the tanh layer grad is (1-t**2) * out.grad\n",
    "# if t = 1 or -1 , we get 0. Means the grad stops to flow back to the network. if t = 0 , the gradient out.grad just passes \n",
    "# through . so the grad flowing thorugh tanh can only decrease from out.grad to 0 . we dont want gradient go to 0 \n",
    "# because that leads to dead neurons. if you have dead neurons, the gradient dont flow, which means the weights and biases\n",
    "# dont update or dont learn.\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(h.abs() > 0.99 , cmap = 'gray', interpolation = 'nearest') # white = true, black = false.\n",
    "# we see a lot of whites, which means all these tanh neurons are getting destroyed (the gradient is not passing through)\n",
    "# all squashing functions have this property. relu, sigmoid, tanh. some alternatives are leaky relu, elu etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in total  11897\n"
     ]
    }
   ],
   "source": [
    "# always run this cell (initialization) before optimization\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.2 # for tanh \n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01 \n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 # for logits\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2 , b2]\n",
    "\n",
    "print('Number of parameters in total ' , sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 4.2304\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32 # to take a minibatch of data \n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb , Yb = Xtr[ix] , Ytr[ix] #batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] #embed chars into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) #concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    h = torch.tanh(hpreact)  # hidden layer\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "  \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # learning rate decay over time\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAE/RJREFUeJzt3X+w5XV93/HnKxjJpDZlcS90RTYLzmpD2uli71CnjopiVUjHxVbNMo1ZDe1KCp1kks4EtFMdZ5xiqmGSsYNdCwUSg6CEup1gzYoYJjNishiCICK7SHRlu7uCQTOkNLu8+8f53uTrcu69595zzr13P/t8zNw53/P5/nrv59x93e/5nO/3e1JVSJLa9SOrXYAkaboMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjnrfaBQCsX7++Nm3atNplSNJx5d577/1uVc0sttyaCPpNmzaxZ8+e1S5Dko4rSf58lOUcupGkxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMatiStjJakVm678/aHtj139Mytcyd/yiF6SGmfQS1LjDHpJatyiQZ/kzCR3JXkoyYNJfqlrPzXJ7iSPdI/ruvYk+a0ke5Pcn+Tl0/5HSJLmN8oR/RHgV6vqp4BXAJcnOQe4ErizqjYDd3bPAS4ENnc/O4BrJ161JGlkiwZ9VR2oqq900z8AHgLOALYCN3aL3Qhc3E1vBW6qgXuAU5JsmHjlkqSRLGmMPskm4Fzgy8DpVXUABn8MgNO6xc4Avt1bbX/XJklaBSMHfZIXALcBv1xV319o0SFtNWR7O5LsSbLn8OHDo5YhSVqikYI+yY8yCPlPVNXvdc0H54ZkusdDXft+4Mze6i8GHj92m1W1s6pmq2p2ZmbRrzyUJC3TolfGJglwHfBQVf1Gb9YuYDtwdff4mV77FUk+CfxT4Km5IR5JOlGt5hWzo9wC4ZXAO4CvJrmva3sPg4C/NcmlwLeAt3Xz7gAuAvYCTwPvmmjFkjRh84UwrO6tCyZl0aCvqj9i+Lg7wAVDli/g8jHrkiRNiFfGSlLjDHpJapxBL0mN8370krQMC32Au9Z4RC9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuM8j16SFnA8nS8/H4/oJalxBr0kNc6gl6TGGfSS1LhFgz7J9UkOJXmg13ZLkvu6n8fmvnkqyaYkf9Wb97FpFi9JWtwoZ93cAHwUuGmuoap+dm46yUeAp3rL76uqLZMqUJI0nlG+SvDuJJuGzeu+OPztwOsmW5YkaVLGHaN/FXCwqh7ptZ2V5E+T/GGSV425fUnSmMa9YOoS4Obe8wPAxqp6Isk/Af5nkp+uqu8fu2KSHcAOgI0bN45ZhiRpPss+ok/yPOBfArfMtVXVM1X1RDd9L7APeOmw9atqZ1XNVtXszMzMcsuQJC1inKGb1wNfr6r9cw1JZpKc1E2fDWwGHh2vREnSOBYduklyM3A+sD7JfuB9VXUdsI0fHrYBeDXwgSRHgKPAZVX15GRLHt1S71Hx2NU/M6VKJGn1jHLWzSXztL9zSNttwG3jlyVJmhSvjJWkxnmbYknNmW/Y9kQdnvWIXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXO0yslHbeWevX7icqg7/HcW0ktcuhGkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNW7RoE9yfZJDSR7otb0/yXeS3Nf9XNSbd1WSvUkeTvLGaRUuSRrNKOfR3wB8FLjpmPZrqurD/YYk5zD4isGfBl4EfD7JS6vq6ARqlaSxnKgXWI3yVYJ3J9k04va2Ap+sqmeAbybZC5wHfGnZFa5hXmAl6Xgwzhj9FUnu74Z21nVtZwDf7i2zv2t7jiQ7kuxJsufw4cNjlCFJWshyg/5a4CXAFuAA8JGuPUOWrWEbqKqdVTVbVbMzMzPLLEOStJhlBX1VHayqo1X1LPBxBsMzMDiCP7O36IuBx8crUZI0jmUFfZINvadvAebOyNkFbEtycpKzgM3AH49XoiRpHIt+GJvkZuB8YH2S/cD7gPOTbGEwLPMY8G6Aqnowya3A14AjwOWecSNJq2uUs24uGdJ83QLLfxD44DhFSZImxytjJalxTXzxSKsXQXievqRJaCLop63VPyTSWuPBzXQ4dCNJjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnBdMTYEXfUhaSzyil6TGeUQv6Tl8V9oWj+glqXEGvSQ1btGgT3J9kkNJHui1/ZckX09yf5Lbk5zStW9K8ldJ7ut+PjbN4iVJixtljP4G4KPATb223cBVVXUkyYeAq4Bf6+btq6otE61S0gnNW4WPZ5SvErw7yaZj2v6g9/Qe4K2TLUuT5odr0olrEmP0vwB8tvf8rCR/muQPk7xqvpWS7EiyJ8mew4cPT6AMSdIwYwV9kvcCR4BPdE0HgI1VdS7wK8DvJvmJYetW1c6qmq2q2ZmZmXHKkCQtYNlBn2Q78C+Af11VBVBVz1TVE930vcA+4KWTKFSStDzLumAqyZsYfPj6mqp6utc+AzxZVUeTnA1sBh6dSKXSEH72IC1u0aBPcjNwPrA+yX7gfQzOsjkZ2J0E4J6qugx4NfCBJEeAo8BlVfXklGqXJI1glLNuLhnSfN08y94G3DZuUZKkyfFeNxrKIZG/ZV/oeGfQS1pxXgC1sgx6aY3ynYQmxaBfQR7FaJr8w6D5GPRaFYbSylnoAMP+PjEY9NJx5nh6Z3g81doy70cvSY3ziF7HBY8MpeUz6E9wrQao49LT4WcrxyeHbiSpcR7RSxpbq+8MW+ERvSQ1ziN66QTmkfiJwaCXJswPLLXWOHQjSY3ziL4xvhWXdKyRgj7J9Qy+H/ZQVf3Dru1U4BZgE/AY8Paq+l4GXzn1m8BFwNPAO6vqK5MvXZos/0iqVaMO3dwAvOmYtiuBO6tqM3Bn9xzgQgbfFbsZ2AFcO36ZkqTlGinoq+pu4Njvft0K3NhN3whc3Gu/qQbuAU5JsmESxUqSlm6cMfrTq+oAQFUdSHJa134G8O3ecvu7tgP9lZPsYHDEz8aNG8coQy05noZPjqdadWKbxlk3GdJWz2mo2llVs1U1OzMzM4UyJEkwXtAfnBuS6R4Pde37gTN7y70YeHyM/UiSxjBO0O8CtnfT24HP9Np/PgOvAJ6aG+KRJK28UU+vvBk4H1ifZD/wPuBq4NYklwLfAt7WLX4Hg1Mr9zI4vfJdE65ZkrQEIwV9VV0yz6wLhixbwOXjFKWF+SGgpKXwFgiS1DiDXpIaZ9BLUuO8qZmWxFvwLp99p9Vi0Guq/OB4cfaRps2hG0lqnEEvSY0z6CWpcY7RayIcZ5bWLo/oJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuOWfXplkpcBt/Sazgb+E3AK8G+Bw137e6rqjmVXKEkay7KDvqoeBrYAJDkJ+A5wO4NvlLqmqj48kQolSWOZ1AVTFwD7qurPk0xok9J0eHGXTjSTGqPfBtzce35FkvuTXJ9k3YT2IUlahrGDPsnzgTcDn+qargVewmBY5wDwkXnW25FkT5I9hw8fHraIJGkCJnFEfyHwlao6CFBVB6vqaFU9C3wcOG/YSlW1s6pmq2p2ZmZmAmVIkoaZRNBfQm/YJsmG3ry3AA9MYB+SpGUa68PYJD8O/HPg3b3mX0+yBSjgsWPmSZJW2FhBX1VPAy88pu0dY1UkSZoor4yVpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDVurC8eAUjyGPAD4ChwpKpmk5wK3AJsYvAtU2+vqu+Nuy9J0tJN6oj+tVW1papmu+dXAndW1Wbgzu65JGkVTGvoZitwYzd9I3DxlPYjSVrEJIK+gD9Icm+SHV3b6VV1AKB7PG0C+5EkLcPYY/TAK6vq8SSnAbuTfH2Ulbo/CjsANm7cOIEyJEnDjH1EX1WPd4+HgNuB84CDSTYAdI+Hhqy3s6pmq2p2ZmZm3DIkSfMYK+iT/J0kf3duGngD8ACwC9jeLbYd+Mw4+5EkLd+4QzenA7cnmdvW71bV/07yJ8CtSS4FvgW8bcz9SJKWaaygr6pHgX88pP0J4IJxti1JmgyvjJWkxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNW7ZQZ/kzCR3JXkoyYNJfqlrf3+S7yS5r/u5aHLlSpKWapxvmDoC/GpVfaX73th7k+zu5l1TVR8evzxJ0riWHfRVdQA40E3/IMlDwBmTKkySNBkTGaNPsgk4F/hy13RFkvuTXJ9k3ST2IUlanrGDPskLgNuAX66q7wPXAi8BtjA44v/IPOvtSLInyZ7Dhw+PW4YkaR5jBX2SH2UQ8p+oqt8DqKqDVXW0qp4FPg6cN2zdqtpZVbNVNTszMzNOGZKkBYxz1k2A64CHquo3eu0beou9BXhg+eVJksY1zlk3rwTeAXw1yX1d23uAS5JsAQp4DHj3WBVKksYyzlk3fwRkyKw7ll+OJGnSvDJWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4qQV9kjcleTjJ3iRXTms/kqSFTSXok5wE/FfgQuAcBl8veM409iVJWti0jujPA/ZW1aNV9f+ATwJbp7QvSdICphX0ZwDf7j3f37VJklbYsr8cfBHDvjS8fmiBZAewo3v6l0keXua+1gPfXea607RW64K1W5t1LY11Lc2arCsfGquunxxloWkF/X7gzN7zFwOP9xeoqp3AznF3lGRPVc2Ou51JW6t1wdqtzbqWxrqW5kSua1pDN38CbE5yVpLnA9uAXVPalyRpAVM5oq+qI0muAD4HnARcX1UPTmNfkqSFTWvohqq6A7hjWtvvGXv4Z0rWal2wdmuzrqWxrqU5YetKVS2+lCTpuOUtECSpccdF0Cd5W5IHkzybZN5Pp+e77UL3ofCXkzyS5JbuA+JJ1HVqkt3ddncnWTdkmdcmua/383+TXNzNuyHJN3vztqxUXd1yR3v73tVrX83+2pLkS93rfX+Sn+3Nm2h/LXabjiQnd//+vV1/bOrNu6prfzjJG8epYxl1/UqSr3X9c2eSn+zNG/qarmBt70xyuFfDv+nN29699o8k2b7CdV3Tq+kbSf6iN28qfZbk+iSHkjwwz/wk+a2u5vuTvLw3b7J9VVVr/gf4KeBlwBeB2XmWOQnYB5wNPB/4M+Ccbt6twLZu+mPAL06orl8HruymrwQ+tMjypwJPAj/ePb8BeOsU+mukuoC/nKd91foLeCmwuZt+EXAAOGXS/bXQ70tvmX8HfKyb3gbc0k2f0y1/MnBWt52TVrCu1/Z+h35xrq6FXtMVrO2dwEeHrHsq8Gj3uK6bXrdSdR2z/L9ncILIVPsMeDXwcuCBeeZfBHyWwXVHrwC+PK2+Oi6O6Kvqoapa7IKqobddSBLgdcCnu+VuBC6eUGlbu+2Nut23Ap+tqqcntP/5LLWuv7Ha/VVV36iqR7rpx4FDwMyE9t83ym06+vV+Grig65+twCer6pmq+iawt9veitRVVXf1fofuYXCdykoY59YmbwR2V9WTVfU9YDfwplWq6xLg5gnte15VdTeDA7v5bAVuqoF7gFOSbGAKfXVcBP2I5rvtwguBv6iqI8e0T8LpVXUAoHs8bZHlt/HcX7APdm/brkly8grX9WNJ9iS5Z244iTXUX0nOY3CEtq/XPKn+GuU2HX+zTNcfTzHon2ne4mOp276UwVHhnGGv6aSMWtu/6l6jTyeZu3ByTfRZN8x1FvCFXvM0+2wh89U98b6a2umVS5Xk88DfHzLrvVX1mVE2MaStFmgfu65Rt9FtZwPwjxhcWzDnKuD/MAizncCvAR9Ywbo2VtXjSc4GvpDkq8D3hyy3Wv3128D2qnq2a152fw3bxZC2Y/+dU/mdWsTI207yc8As8Jpe83Ne06raN2z9KdX2v4Cbq+qZJJcxeEf0uhHXnWZdc7YBn66qo722afbZQlbs92vNBH1VvX7MTcx324XvMnhL9LzuqOw5t2NYbl1JDibZUFUHumA6tMCm3g7cXlV/3dv2gW7ymST/A/gPK1lXNzRCVT2a5IvAucBtrHJ/JfkJ4PeB/9i9pZ3b9rL7a4hFb9PRW2Z/kucBf4/BW/FR1p1mXSR5PYM/nq+pqmfm2ud5TScVWqPc2uSJ3tOPAx/qrXv+Met+caXq6tkGXN5vmHKfLWS+uifeVy0N3Qy97UINPt24i8H4OMB2YJR3CKPY1W1vlO0+Z1ywC7u5cfGLgaGfzk+jriTr5oY+kqwHXgl8bbX7q3vtbmcwdvmpY+ZNsr9GuU1Hv963Al/o+mcXsC2Ds3LOAjYDfzxGLUuqK8m5wH8D3lxVh3rtQ1/TCdU1am0bek/fDDzUTX8OeENX4zrgDfzwu9up1tXV9jIGH25+qdc27T5byC7g57uzb14BPNUdzEy+r6bxafOkf4C3MPgr9wxwEPhc1/4i4I7echcB32Dw1/i9vfazGfxH3At8Cjh5QnW9ELgTeKR7PLVrnwX+e2+5TcB3gB85Zv0vAF9lEFi/A7xgpeoC/lm37z/rHi9dC/0F/Bzw18B9vZ8t0+ivYb8vDIaC3txN/1j379/b9cfZvXXf2633MHDhhH/fF6vr893/g7n+2bXYa7qCtf1n4MGuhruAf9Bb9xe6vtwLvGsl6+qevx+4+pj1ptZnDA7sDnS/z/sZfJ5yGXBZNz8MvqBpX7fv2d66E+0rr4yVpMa1NHQjSRrCoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXH/H6gFZgxyoMlQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(h.view(-1).tolist() , 50); # much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2afc3cdefd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(h.abs() > 0.99 , cmap = 'gray', interpolation = 'nearest') # lesser neurons are saturating. because our\n",
    "# initialization is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in total  11897\n"
     ]
    }
   ],
   "source": [
    "# initialization becomes difficult as your NN grows. no one does it by hand.\n",
    "# there are research papers that dictate what should the values be for initialization. paper by kaiming he\n",
    "# if you properly initialize the forward pass, the backprop works good as well for large NN\n",
    "# torch.nn.kaiming\n",
    "# also we define gain. gain is boost required to get the data back close the input std deviation (since tanh squshes the \n",
    "# input, it changes the st dev of data. we want the std dev to be same at all times. so we boost after a squashing layer) \n",
    "# https://pytorch.org/docs/stable/nn.init.html\n",
    "\n",
    "# always run this cell (initialization) before optimization\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3) / (n_embd * block_size)**0.5) # for tanh. kaiming init\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01 \n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 # for logits\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2 , b2]\n",
    "\n",
    "print('Number of parameters in total ' , sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we study batch normalization \n",
    "# we dont want hidden states to be too small or too large. we want to roughly gaussian at init.\n",
    "# BN says that you take the hidden state and normalize them to get unit gaussian state. these operations are differentiable\n",
    "# so works for backward prop too\n",
    "hmean, hstd = hpreact.mean(0, keepdims = True) , hpreact.std(0, keepdims = True)\n",
    "(hpreact - hmean) / hstd # every neuron now has unit gaussian state i.e The distribution has zero mean and variance equal to 1\n",
    "\n",
    "# problem is we want this at initialization only, not at all times. we want this distribution to move around\n",
    "# so we do a scale(bngain) and shift(bnbias) \n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "hpreact = bngain * (hpreact - hpreact.mean(0, keepdims = True)) / hpreact.std(0, keepdims = True) + bnbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in total  12297\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3) / (n_embd * block_size)**0.5) # for tanh. kaiming init\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01 \n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 # for logits\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2 , b2 , bngain , bnbias]\n",
    "\n",
    "print('Number of parameters in total ' , sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 4.2651\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32 # to take a minibatch of data \n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb , Yb = Xtr[ix] , Ytr[ix] #batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] #embed chars into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) #concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdims = True)) / hpreact.std(0, keepdims = True) + bnbias # BN\n",
    "    \n",
    "    h = torch.tanh(hpreact)  # hidden layer\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "  \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # learning rate decay over time\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in total  12297\n"
     ]
    }
   ],
   "source": [
    "# the problem now is that at prediction, we are dependent on a batch of data to calculate mean and std.\n",
    "# but at test time we only get one example. so how do we cal mean and std ?\n",
    "# solution is to keep the mean and std of the batch in a running manner during training\n",
    "\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3) / (n_embd * block_size)**0.5) # for tanh. kaiming init\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01 \n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 # for logits\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "# Batch normalization\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "#keep track of running mean and std\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2 , b2 , bngain , bnbias]\n",
    "\n",
    "print('Number of parameters in total ' , sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a39f94aa0ba2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m100000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.01\u001b[0m \u001b[1;31m# learning rate decay over time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32 # to take a minibatch of data \n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb , Yb = Xtr[ix] , Ytr[ix] #batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] #embed chars into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) #concatenate the vectors\n",
    "    \n",
    "    hpreact = embcat @ W1 # hidden layer pre-activation. dont need the b1 bias now, since we now have bnbias\n",
    "    \n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    \n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias # BN\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani # keep bnmean_running almost same, but slightly update with new batch mean\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "    h = torch.tanh(hpreact)  # hidden layer\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "  \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # learning rate decay over time\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "    emb = C[x] # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1\n",
    "    hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias # now we can simply use the bnmean_running and bnstd_running we tracked during training\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY + PYTORCHIFYING -----------\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5  # kaiming init\n",
    "        self.bias = torch.zeros(fan_out) if bias else None # whether or not we want a bias for linear layer\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "    \n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps # from batcnorm paper, to make sure we dont divide by zero\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True) # batch mean\n",
    "            xvar = x.var(0, keepdim=True) # batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta # same as scale(bngain) and shift(bnbias)\n",
    "        \n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[-1].weight *= 0.1 # make output softmax/logit a less confident\n",
    "    # all other layers: apply gain\n",
    "    for layer in layers[:-1]: # apply a gain of 5/3 to every tanh layer\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3 # weights were initialized with torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5. so we just need to multiply by 5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    for layer in layers:\n",
    "        x = layer(x) # invokes the __call__ method of each class in layer\n",
    "    loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "    # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "  \n",
    "    # update\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "\n",
    "    break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms of activations. The reason saturation is good for tanh layers of our kaiming. Note : all layers\n",
    "# are well behaved , they follow the same graph\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms of gradients\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.grad\n",
    "        print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # last layer: make less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "    #layers[-1].weight *= 0.1\n",
    "    # all other layers: apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = [] # to keep track of update to data ratio\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    for layer in layers:\n",
    "        x = layer(x) # invokes the __call__ method of each class in layer\n",
    "    loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "    # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "  \n",
    "    # update\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters]) # update std : lr*p.grad.std() to data std : p.data.std() for every parameter\n",
    "    \n",
    "    if i >= 1000:\n",
    "        break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "        legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
